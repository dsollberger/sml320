---
title: "8: MCMC"
author: "Derek Sollberger"
date: "2024-02-22"
# execute:
#   cache: true
# format:
#   revealjs:
#     scrollable: true
format:
  html:
    toc: true
params:
  heavy_chunks: "true"
  # heavy_chunks: "false"
---

\newcommand{\ds}{\displaystyle}

```{r}
#| message: false
#| warning: false

library("bayesplot")
library("ggtext")
library("rstan")
library("patchwork")
library("tidyverse")

knitr::opts_chunk$set(echo = TRUE)

# tips_df <- readr::read_csv("tips.csv")
```

# A Good Example

:::: {.panel-tabset}

## Define Model

```{r}
#| echo: true
# STEP 1: DEFINE the model
bb_model <- "
  data {
    int<lower = 0, upper = 9> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(9, pi);
    pi ~ beta(2, 2);
  }
"
```

## Simulate Posterior

```{r}
#| eval: !expr params$heavy_chunks
#| message: false
#| warning: false
start_time <- Sys.time()

# STEP 2: SIMULATE the posterior
good_simulation <- stan(model_code = bb_model, data = list(Y = 4), 
                        chains = 4, iter = 5000*2, seed = 84735)

end_time <- Sys.time()
print(round(end_time- start_time))
```

## Histogram

```{r}
#| eval: !expr params$heavy_chunks
#| message: false
#| warning: false
bayesplot::mcmc_hist(good_simulation, pars = "pi")
```

## Density

```{r}
#| eval: !expr params$heavy_chunks
#| message: false
#| warning: false
bayesplot::mcmc_dens(good_simulation, pars = "pi") + 
  stat_function(fun = dbeta, args = list(7, 8),
                color = "#E77500", linewidth = 3) + 
  labs(title = "MCMC: <span style='color:#619CFF'>simulation</span> versus <span style='color:#E77500'>theoretical</span>",
         subtitle = "Beta-Binomial Example",
         caption = "SML 320") +
  theme_minimal() +
  theme(plot.title = element_markdown())
```

## Trace

```{r}
#| eval: !expr params$heavy_chunks
#| message: false
#| warning: false
bayesplot::mcmc_trace(good_simulation, pars = "pi") + 
  labs(title = "MCMC Trace",
         subtitle = "Good Example",
         caption = "SML 320") +
  theme_minimal() +
  theme(plot.title = element_markdown())
```

::::


# A Bad Example

:::: {.panel-tabset}

## Define Model

```{r}
#| echo: true
# STEP 1: DEFINE the model
bb_model <- "
  data {
    int<lower = 0, upper = 9> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(9, pi);
    pi ~ beta(2, 2);
  }
"
```

## Simulate Posterior

```{r}
#| eval: !expr params$heavy_chunks
#| message: false
#| warning: false
start_time <- Sys.time()

# STEP 2: SIMULATE the posterior
bad_simulation <- stan(model_code = bb_model, data = list(Y = 4), 
                        chains = 4, iter = 50*2, seed = 84735)

end_time <- Sys.time()
print(round(end_time- start_time))
```

## Histogram

```{r}
#| eval: !expr params$heavy_chunks
#| message: false
#| warning: false
bayesplot::mcmc_hist(bad_simulation, pars = "pi")
```

## Density

```{r}
#| eval: !expr params$heavy_chunks
#| message: false
#| warning: false
bayesplot::mcmc_dens(bad_simulation, pars = "pi") + 
  stat_function(fun = dbeta, args = list(7, 8),
                color = "#E77500", linewidth = 3) + 
  labs(title = "MCMC: <span style='color:#619CFF'>simulation</span> versus <span style='color:#E77500'>theoretical</span>",
         subtitle = "Beta-Binomial Example",
         caption = "SML 320") +
  theme_minimal() +
  theme(plot.title = element_markdown())
```

## Trace

```{r}
#| eval: !expr params$heavy_chunks
#| message: false
#| warning: false
bayesplot::mcmc_trace(bad_simulation, pars = "pi") + 
  labs(title = "MCMC Trace",
         subtitle = "Bad Example",
         caption = "SML 320") +
  theme_minimal() +
  theme(plot.title = element_markdown())
```

::::

# Metropolis Algorithm

With a *symmetric* proposal model

$$q(\mu^{′}|\mu) = q(\mu|\mu^{′})$$

the probability of accepting a move from a current location $\mu$ to a proposed location $\mu^{′}$ comes down to a comparison of their posterior plausibility: $f(\mu^{'}|y)$ versus $f(\mu|y)$. There are two possible scenarios here:

* Scenario 1: $f(\mu^{'}|y) \geq f(\mu|y)$.  When the posterior plausibility of $\mu^{′}$ is at least as great as that of $\mu$, $\alpha=1$. Thus, we’ll *definitely* move there.
* Scenario 2: $f(\mu^{'}|y) < f(\mu|y)$.  If the posterior plausibility of $\mu^{′}$ is less than that of $\mu$, then

$$α=\ds\frac{f(\mu^{′}|y)}{f(\mu|y)}<1$$

Thus, we *might* move there.

# Gibbs Sampling

When Hastings generalized the Metropolis Algorithm, the need for a symmetric proposal model was removed.

* Step 1: *Propose a new location.* Conditioned on the current location $\mu$, draw a location $\mu^{′}$ from a proposal model with pdf $q(\mu^{′}|\mu)$.
* Step 2: *Decide whether or not to go there.*

    * Calculate the **acceptance probability** (i.e., the probability of accepting the proposal $\mu^{′}$):
    $$\alpha = \text{min}\left\{1, \ds\frac{f(\mu^{′}) \cdot L(\mu^{′}|y)}{f(\mu) \cdot L(\mu|y)} \cdot \ds\frac{q(\mu^{′}|\mu)}{q(\mu|\mu^{′})} \right\}$$
    * Figuratively, flip a weighted coin. If it’s Heads, with probability $\alpha$, go to the proposed location $\mu^{′}$. If it’s Tails, with probability $1−\alpha$, stay at $\mu$:
    $$\mu^{(i+1)} = \begin{cases}
      \mu^{'} & \text{with probability } \alpha \\
      \mu & \text{with probability } 1-\alpha \\
    \end{cases}$$
:::

# Hamiltonian Monte Carlo




# Footnotes

::: {.callout-note collapse="true"}
## Session Info

```{r}
sessionInfo()
```
:::


:::: {.columns}

::: {.column width="50%"}
	
:::

::: {.column width="50%"}

:::

::::

:::: {.panel-tabset}



::::